{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMS3V3oKoQPV2VSaoBRttzk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurefindele-o-catto/ML-Projects/blob/main/CIFAR-10/Model%20using%20resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WG0dQeDBS0UJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "zQbny3t5Zcyx",
        "outputId": "b9d64d29-1a2d-439e-fde0-5e195aff6b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "GPU name: No GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "1_4zJrG1JaMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha = 0.1):\n",
        "  if alpha <= 0:\n",
        "    return x, y, y, 1.0\n",
        "\n",
        "  lam = np.random.beta(alpha, alpha)\n",
        "  batch_size = x.size(0)\n",
        "  index = torch.randperm(batch_size).to(x.device)\n",
        "  mixed_x = lam*x + (1-lam) * x[index, :]\n",
        "  y_a, y_b = y, y[index]\n",
        "\n",
        "  return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def cutmix_data(x, y, alpha = 1.0):\n",
        "  if alpha <= 0:\n",
        "    return x, y, y, 1.0\n",
        "\n",
        "  lam = np.random.beta(alpha, alpha)\n",
        "  batch_size, _, H, W = x.size()\n",
        "  index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "  cut_rat = np.sqrt(1. - lam)\n",
        "  cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
        "  cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "  x1, x2 = np.clip(cx - cut_w // 2, 0, W), np.clip(cx + cut_w // 2, 0, W)\n",
        "  y1, y2 = np.clip(cy - cut_h // 2, 0, H), np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "  x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
        "\n",
        "  lam = 1 - ((x2 - x1) * (y2-y1) / (W*H) )\n",
        "  y_a, y_b = y, y[index]\n",
        "\n",
        "  return x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
        "  return lam * criterion(pred, y_a) + (1 - lam)*criterion(pred, y_b)"
      ],
      "metadata": {
        "id": "yLc8iCbBJcBi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Using Resnet**"
      ],
      "metadata": {
        "id": "Ui80tGC8Zo2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Layers\n",
        "- BasicBlock: Each block has two 3Ã—3 convolutions with BatchNorm + ReLU, plus a skip connection that adds the input back to the output.\n",
        "- Skip connections: Solve the vanishing gradient problem by letting gradients flow directly backward.\n",
        "- ResNet18 structure:\n",
        "- Conv1: 3Ã—3 conv (CIFAR version) â†’ 64 channels\n",
        "- Stage 1: 2 blocks, 64 channels\n",
        "- Stage 2: 2 blocks, 128 channels (downsample)\n",
        "- Stage 3: 2 blocks, 256 channels (downsample)\n",
        "- Stage 4: 2 blocks, 512 channels (downsample)\n",
        "- Global Average Pooling â†’ Fully Connected layer (10 classes for CIFARâ€‘10).\n",
        "\n",
        "Early layers learn edges/textures, middle layers learn parts (wheels, eyes), deeper layers learn object semantics."
      ],
      "metadata": {
        "id": "pwydLYjoqqYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  def __init__(self, in_ch, out_ch, stride = 1):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride = stride, padding = 1, bias  = False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_ch)\n",
        "    self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    self.downsample = None\n",
        "    if stride != 1 or in_ch != out_ch:\n",
        "      self.downsample = nn.Sequential(\n",
        "          nn.Conv2d(in_ch, out_ch, kernel_size = 1, stride = stride, bias = False),\n",
        "          nn.BatchNorm2d(out_ch)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x  #save input for skip connection\n",
        "\n",
        "    out = F.relu(self.bn1(self.conv1(x)), inplace = True)\n",
        "    out = self.bn2(self.conv2(out))\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out = F.relu(out + identity, inplace = True)  #residual connection\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "3SZysPdqnEjY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " class ResNet_CIFAR(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, layers=(2,2,2,2), num_classes=10):\n",
        "        super().__init__()\n",
        "        # CIFAR stem: 3x3 conv, stride 1, no maxpool\n",
        "        self.in_ch = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64,  layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, out_ch, blocks, stride):\n",
        "        layers = [block(self.in_ch, out_ch, stride)]\n",
        "        self.in_ch = out_ch * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_ch, out_ch, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def resnet18_cifar(num_classes=10):\n",
        "    return ResNet_CIFAR(BasicBlock, (2,2,2,2), num_classes)"
      ],
      "metadata": {
        "id": "N9qubo6YnxNr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4456)\n",
        "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding = 4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "])\n",
        "\n",
        "transform_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "])\n",
        "\n",
        "trainset = tv.datasets.CIFAR10(root='./data', train = True, download = True, transform = transform_train)\n",
        "testset = tv.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 128, shuffle = True, num_workers = 2, pin_memory = True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size = 256, shuffle = False, num_workers = 2, pin_memory = True)"
      ],
      "metadata": {
        "id": "Dcluv5ojaD9F",
        "outputId": "7fde381c-2e36-41a7-c0df-e44983c726af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:02<00:00, 63.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Setup**"
      ],
      "metadata": {
        "id": "YlvAwesYcyhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, targets):\n",
        "  return (logits.argmax(1) == targets).float().mean().item() * 100.0\n",
        "\n",
        "def train_resnet(model, train_loader, test_loader, epochs = 200, base_lr = 0.1, weight_decay = 5e-4, label_smoothing = 0.1, device = device, save_dir=\"checkpoints\"):\n",
        "  model = model.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = base_lr, momentum=0.9, weight_decay = weight_decay, nesterov = True)\n",
        "  scheduler = torch.optimc.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs, eta_min = base_lr*1e-2)\n",
        "\n",
        "  train_hist, test_hist = [], []\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    total, correct, running = 0, 0, 0.0\n",
        "    for x, y in train_loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(x)\n",
        "      loss = criterion(logits, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running += loss.item() * x.size(0)\n",
        "      total += x.size(0)\n",
        "      correct += (logits.argmax(1) == y).sum().item()\n",
        "\n",
        "    train_loss = running/total\n",
        "    train_acc = 100.0 * correct/total\n",
        "\n",
        "    model.eval()\n",
        "    total, correct, runnin = 0, 0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        running += loss.item() * x.size(0)\n",
        "        total += x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "\n",
        "    test_loss = running/total\n",
        "    test_acc = 100.0*correct/total\n",
        "\n",
        "    scheduler.step()\n",
        "    train_hist.append(train_acc)\n",
        "    test_hist.append(test_acc)\n",
        "    print(f\"Epoch [{epoch:3d}/{epochs}] LR {scheduler.get_last_lr()[0]:.5f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "  return train_hist, test_hist"
      ],
      "metadata": {
        "id": "mxRMTjIRc_0u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_resnet_adv(model, train_loader, test_loader, epochs=200, base_lr=0.1,\n",
        "                     weight_decay=5e-4, label_smoothing=0.1,\n",
        "                     use_mixup=False, use_cutmix=False, alpha=1.0, device=None, save_dir = \"checkpoints\"):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9,\n",
        "                                weight_decay=weight_decay, nesterov=True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs, eta_min = base_lr*1e-2)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    train_hist, test_hist = [], []\n",
        "    best_acc = 0.0   # track best test accuracy\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total, correct = 0, 0\n",
        "\n",
        "        mix_count, cut_count, none_count = 0, 0, 0\n",
        "\n",
        "        if epoch < int(0.8*epochs):   #first 80% epochs\n",
        "          p_mix, p_cut, p_none = 0.4, 0.4, 0.2\n",
        "          alpha_now = alpha\n",
        "        else:\n",
        "          p_mix, p_cut, p_none = 0.15, 0.15, 0.70\n",
        "          alpha_now = 0.2\n",
        "\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            r = random.random()\n",
        "\n",
        "            # Apply MixUp or CutMix\n",
        "            if r < p_mix:\n",
        "                x, y_a, y_b, lam = mixup_data(x, y, alpha_now)\n",
        "                mix_count += 1\n",
        "            elif r < p_mix + p_cut:\n",
        "                x, y_a, y_b, lam = cutmix_data(x, y, alpha_now)\n",
        "                cut_count += 1\n",
        "            else:\n",
        "                y_a, y_b, lam = y, y, 1.0\n",
        "                none_count += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = mixup_cutmix_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += y.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100.0 * correct / total\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        total, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                total += y.size(0)\n",
        "                correct += (logits.argmax(1) == y).sum().item()\n",
        "        test_acc = 100.0 * correct / total\n",
        "\n",
        "        train_hist.append(train_acc)\n",
        "        test_hist.append(test_acc)\n",
        "        print(f\"Epoch [{epoch:3d}/{epochs}] \"\n",
        "              f\"LR {scheduler.get_last_lr()[0]:.5f} | \"\n",
        "              f\"Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}% | \"\n",
        "              f\"MixUp: {mix_count}, CutMix: {cut_count}, None: {none_count}\")\n",
        "\n",
        "\n",
        "        # Save latest checkpoint every N epochs (resume safety)\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save({\n",
        "              \"epoch\": epoch,\n",
        "              \"model_state\": model.state_dict(),\n",
        "              \"optimizer_state\": optimizer.state_dict(),\n",
        "              \"scheduler_state\": scheduler.state_dict(),\n",
        "              \"train_hist\": train_hist,\n",
        "              \"test_hist\": test_hist,\n",
        "              \"best_acc\": best_acc,\n",
        "            }, os.path.join(save_dir, f\"checkpoint_epoch{epoch}.pth\"))\n",
        "        print(f\"ðŸ’¾ Saved checkpoint at epoch {epoch}\")\n",
        "\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"scheduler_state\": scheduler.state_dict(),\n",
        "                \"train_hist\": train_hist,\n",
        "                \"test_hist\": test_hist,\n",
        "                \"best_acc\": best_acc,\n",
        "            }, os.path.join(save_dir, \"best_model.pth\"))\n",
        "\n",
        "            print(f\"ðŸŒŸ New best model saved at epoch {epoch} with acc {best_acc:.2f}%\")\n",
        "\n",
        "    return train_hist, test_hist"
      ],
      "metadata": {
        "id": "tzfkeh6jldoN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18_cifar()\n",
        "# Randomize CutMix and MixUp\n",
        "train_hist, test_hist = train_resnet_adv(model, train_loader, test_loader,\n",
        "                                         epochs=200, base_lr=0.1,\n",
        "                                         use_mixup=True, use_cutmix=True, alpha=1.0)"
      ],
      "metadata": {
        "id": "3aSwKF3OlvDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "42d02a5c-797c-49aa-d8c0-16641b99813d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3146377080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18_cifar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Randomize CutMix and MixUp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_hist, test_hist = train_resnet_adv(model, train_loader, test_loader,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          use_mixup=True, use_cutmix=True, alpha=1.0)\n",
            "\u001b[0;32m/tmp/ipython-input-2459802025.py\u001b[0m in \u001b[0;36mtrain_resnet_adv\u001b[0;34m(model, train_loader, test_loader, epochs, base_lr, weight_decay, label_smoothing, use_mixup, use_cutmix, alpha, device, save_dir)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_cutmix_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"checkpoints/best_model.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
        "start_epoch = checkpoint[\"epoch\"] + 1\n",
        "best_acc = checkpoint[\"best_acc\"]\n",
        "train_hist = checkpoint[\"train_hist\"]\n",
        "test_hist = checkpoint[\"test_hist\"]\n",
        "\n",
        "print(f\"Resumed from epoch {start_epoch}, best acc {best_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "LLor9tlLoXPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix & Misclassifications**"
      ],
      "metadata": {
        "id": "vDfaV8a6r1SA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sZFuvKmr56n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion & Analysis**"
      ],
      "metadata": {
        "id": "qd9s7dVBcmlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used:\n",
        "\n",
        " *Loss Function*\n",
        " - CrossEntropyLoss with Label Smoothing:\n",
        "- Standard CE compares predicted logits vs. oneâ€‘hot labels.\n",
        "- Label smoothing (e.g., 0.1) softens the target distribution: instead of [0,0,1,0,...], the true class gets 0.9 and others share 0.1.\n",
        "- Benefits: prevents overconfidence, improves calibration, helps generalization.\n",
        "\n",
        "\n",
        "*Optimizer*\n",
        "- SGD with Momentum (0.9) + Nesterov:\n",
        "- SGD updates weights in the direction of the gradient.\n",
        "- Momentum accumulates past gradients â†’ smoother, faster convergence.\n",
        "- Nesterov momentum looks ahead, correcting overshoot.\n",
        "\n",
        "\n",
        "*Learning Rate Scheduler*\n",
        "- CosineAnnealingLR:\n",
        "- Starts at base LR (0.1).\n",
        "- Decays smoothly following a cosine curve toward eta_min (0.001).\n",
        "- Prevents sudden drops, encourages better minima.\n",
        "\n",
        "\n",
        "*Weight Decay*\n",
        "- L2 regularization: adds a penalty proportional to the square of weights.\n",
        "- Prevents weights from growing too large, reduces overfitting.\n",
        "- Standard value 5eâ€‘4.\n",
        "\n",
        "\n",
        "\n",
        "*CutMix and MixUp*\n",
        "- MixUp: blends two images and their labels linearly. Encourages smooth decision boundaries.\n",
        "- CutMix: replaces a patch of one image with another, labels mixed by patch area. Preserves natural textures.\n",
        "- MixUp teaches interpolation, CutMix teaches occlusion robustness. Alternating gives the model both benefits.\n",
        "- Tapering: Strong mixing early (0.4, 0.4, 0.2), weaker late (0.15, 0.15, 0.70) so the model sharpens on real labels.\n"
      ],
      "metadata": {
        "id": "Dd5dxYGscqqp"
      }
    }
  ]
}