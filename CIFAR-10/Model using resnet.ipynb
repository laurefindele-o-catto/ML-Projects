{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQ3x0nUYbmTze338vcA5OU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurefindele-o-catto/ML-Projects/blob/main/CIFAR-10/Model%20using%20resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WG0dQeDBS0UJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "zQbny3t5Zcyx",
        "outputId": "22aea777-390f-4491-d0bd-1942335c0fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "GPU name: No GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "1_4zJrG1JaMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha = 0.1):\n",
        "  if alpha <= 0:\n",
        "    return x, y, y, 1.0\n",
        "\n",
        "  lam = np.random.beta(alpha, alpha)\n",
        "  batch_size = x.size(0)\n",
        "  index = torch.randperm(batch_size).to(x.device)\n",
        "  mixed_x = lam*x + (1-lam) * x[index, :]\n",
        "  y_a, y_b = y, y[index]\n",
        "\n",
        "  return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def cutmix_data(x, y, alpha = 1.0):\n",
        "  if alpha <= 0:\n",
        "    return x, y, y, 1.0\n",
        "\n",
        "  lam = np.random.beta(alpha, alpha)\n",
        "  batch_size, _, H, W = x.size()\n",
        "  index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "  cut_rat = np.sqrt(1. - lam)\n",
        "  cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
        "  cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "  x1, x2 = np.clip(cx - cut_w // 2, 0, W), np.clip(cx + cut_w // 2, 0, W)\n",
        "  y1, y2 = np.clip(cy - cut_h // 2, 0, H), np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "  x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n",
        "\n",
        "  lam = 1 - ((x2 - x1) * (y2-y1) / (W*H) )\n",
        "  y_a, y_b = y, y[index]\n",
        "\n",
        "  return x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_cutmix_criterion(criterion, pred, y_a, y_b, lam):\n",
        "  return lam * criterion(pred, y_a) + (1 - lam)*criterion(pred, y_b)"
      ],
      "metadata": {
        "id": "yLc8iCbBJcBi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Using Resnet**"
      ],
      "metadata": {
        "id": "Ui80tGC8Zo2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Layers\n",
        "- BasicBlock: Each block has two 3×3 convolutions with BatchNorm + ReLU, plus a skip connection that adds the input back to the output.\n",
        "- Skip connections: Solve the vanishing gradient problem by letting gradients flow directly backward.\n",
        "- ResNet18 structure:\n",
        "- Conv1: 3×3 conv (CIFAR version) → 64 channels\n",
        "- Stage 1: 2 blocks, 64 channels\n",
        "- Stage 2: 2 blocks, 128 channels (downsample)\n",
        "- Stage 3: 2 blocks, 256 channels (downsample)\n",
        "- Stage 4: 2 blocks, 512 channels (downsample)\n",
        "- Global Average Pooling → Fully Connected layer (10 classes for CIFAR‑10).\n",
        "\n",
        "Early layers learn edges/textures, middle layers learn parts (wheels, eyes), deeper layers learn object semantics."
      ],
      "metadata": {
        "id": "pwydLYjoqqYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  def __init__(self, in_ch, out_ch, stride = 1):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride = stride, padding = 1, bias  = False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_ch)\n",
        "    self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    self.downsample = None\n",
        "    if stride != 1 or in_ch != out_ch:\n",
        "      self.downsample = nn.Sequential(\n",
        "          nn.Conv2d(in_ch, out_ch, kernel_size = 1, stride = stride, bias = False),\n",
        "          nn.BatchNorm2d(out_ch)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x  #save input for skip connection\n",
        "\n",
        "    out = F.relu(self.bn1(self.conv1(x)), inplace = True)\n",
        "    out = self.bn2(self.conv2(out))\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out = F.relu(out + identity, inplace = True)  #residual connection\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "3SZysPdqnEjY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " class ResNet_CIFAR(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, layers=(2,2,2,2), num_classes=10):\n",
        "        super().__init__()\n",
        "        # CIFAR stem: 3x3 conv, stride 1, no maxpool\n",
        "        self.in_ch = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64,  layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, out_ch, blocks, stride):\n",
        "        layers = [block(self.in_ch, out_ch, stride)]\n",
        "        self.in_ch = out_ch * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_ch, out_ch, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def resnet18_cifar(num_classes=10):\n",
        "    return ResNet_CIFAR(BasicBlock, (2,2,2,2), num_classes)"
      ],
      "metadata": {
        "id": "N9qubo6YnxNr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4456)\n",
        "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform_train = T.Compose([\n",
        "    T.RandomCrop(32, padding = 4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "])\n",
        "\n",
        "transform_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
        "])\n",
        "\n",
        "trainset = tv.datasets.CIFAR10(root='./data', train = True, download = True, transform = transform_train)\n",
        "testset = tv.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 128, shuffle = True, num_workers = 2, pin_memory = True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size = 256, shuffle = False, num_workers = 2, pin_memory = True)"
      ],
      "metadata": {
        "id": "Dcluv5ojaD9F",
        "outputId": "d58ce06b-fc9d-4c1a-e911-59cc678d66a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 46.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Setup**"
      ],
      "metadata": {
        "id": "YlvAwesYcyhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, targets):\n",
        "  return (logits.argmax(1) == targets).float().mean().item() * 100.0\n",
        "\n",
        "def train_resnet(model, train_loader, test_loader, epochs = 200, base_lr = 0.1, weight_decay = 5e-4, label_smoothing = 0.1, device = device, save_dir=\"checkpoints\"):\n",
        "  model = model.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = base_lr, momentum=0.9, weight_decay = weight_decay, nesterov = True)\n",
        "  scheduler = torch.optimc.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs, eta_min = base_lr*1e-2)\n",
        "\n",
        "  train_hist, test_hist = [], []\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    total, correct, running = 0, 0, 0.0\n",
        "    for x, y in train_loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(x)\n",
        "      loss = criterion(logits, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running += loss.item() * x.size(0)\n",
        "      total += x.size(0)\n",
        "      correct += (logits.argmax(1) == y).sum().item()\n",
        "\n",
        "    train_loss = running/total\n",
        "    train_acc = 100.0 * correct/total\n",
        "\n",
        "    model.eval()\n",
        "    total, correct, runnin = 0, 0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        running += loss.item() * x.size(0)\n",
        "        total += x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "\n",
        "    test_loss = running/total\n",
        "    test_acc = 100.0*correct/total\n",
        "\n",
        "    scheduler.step()\n",
        "    train_hist.append(train_acc)\n",
        "    test_hist.append(test_acc)\n",
        "    print(f\"Epoch [{epoch:3d}/{epochs}] LR {scheduler.get_last_lr()[0]:.5f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "  return train_hist, test_hist"
      ],
      "metadata": {
        "id": "mxRMTjIRc_0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_resnet_adv(model, train_loader, test_loader, epochs=200, base_lr=0.1,\n",
        "                     weight_decay=5e-4, label_smoothing=0.1,\n",
        "                     use_mixup=False, use_cutmix=False, alpha=1.0, device=None, save_dir = \"checkpoints\"):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9,\n",
        "                                weight_decay=weight_decay, nesterov=True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs, eta_min = base_lr*1e-2)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    train_hist, test_hist = [], []\n",
        "    best_acc = 0.0   # track best test accuracy\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total, correct = 0, 0\n",
        "\n",
        "        mix_count, cut_count, none_count = 0, 0, 0\n",
        "\n",
        "        if epoch < int(0.8*epochs):   #first 80% epochs\n",
        "          p_mix, p_cut, p_none = 0.4, 0.4, 0.2\n",
        "          alpha_now = alpha\n",
        "        else:\n",
        "          p_mix, p_cut, p_none = 0.15, 0.15, 0.70\n",
        "          alpha_now = 0.2\n",
        "\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            r = random.random()\n",
        "\n",
        "            # Apply MixUp or CutMix\n",
        "            if r < p_mix:\n",
        "                x, y_a, y_b, lam = mixup_data(x, y, alpha_now)\n",
        "                mix_count += 1\n",
        "            elif r < p_mix + p_cut:\n",
        "                x, y_a, y_b, lam = cutmix_data(x, y, alpha_now)\n",
        "                cut_count += 1\n",
        "            else:\n",
        "                y_a, y_b, lam = y, y, 1.0\n",
        "                none_count += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = mixup_cutmix_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += y.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100.0 * correct / total\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        total, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                total += y.size(0)\n",
        "                correct += (logits.argmax(1) == y).sum().item()\n",
        "        test_acc = 100.0 * correct / total\n",
        "\n",
        "        train_hist.append(train_acc)\n",
        "        test_hist.append(test_acc)\n",
        "        print(f\"Epoch [{epoch:3d}/{epochs}] \"\n",
        "              f\"LR {scheduler.get_last_lr()[0]:.5f} | \"\n",
        "              f\"Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}% | \"\n",
        "              f\"MixUp: {mix_count}, CutMix: {cut_count}, None: {none_count}\")\n",
        "\n",
        "\n",
        "        # Save latest checkpoint every N epochs (resume safety)\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save({\n",
        "              \"epoch\": epoch,\n",
        "              \"model_state\": model.state_dict(),\n",
        "              \"optimizer_state\": optimizer.state_dict(),\n",
        "              \"scheduler_state\": scheduler.state_dict(),\n",
        "              \"train_hist\": train_hist,\n",
        "              \"test_hist\": test_hist,\n",
        "              \"best_acc\": best_acc,\n",
        "            }, os.path.join(save_dir, f\"checkpoint_epoch{epoch}.pth\"))\n",
        "        print(f\"💾 Saved checkpoint at epoch {epoch}\")\n",
        "\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"scheduler_state\": scheduler.state_dict(),\n",
        "                \"train_hist\": train_hist,\n",
        "                \"test_hist\": test_hist,\n",
        "                \"best_acc\": best_acc,\n",
        "            }, os.path.join(save_dir, \"best_model.pth\"))\n",
        "\n",
        "            print(f\"🌟 New best model saved at epoch {epoch} with acc {best_acc:.2f}%\")\n",
        "\n",
        "    return train_hist, test_hist"
      ],
      "metadata": {
        "id": "tzfkeh6jldoN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18_cifar()\n",
        "# Randomize CutMix and MixUp\n",
        "train_hist, test_hist = train_resnet_adv(model, train_loader, test_loader,\n",
        "                                         epochs=200, base_lr=0.1,\n",
        "                                         use_mixup=True, use_cutmix=True, alpha=1.0)"
      ],
      "metadata": {
        "id": "3aSwKF3OlvDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36762484-9540-4c26-f063-36e13e877097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [  1/200] LR 0.09999 | Train Acc: 18.84% | Test Acc: 35.97% | MixUp: 166, CutMix: 149, None: 76\n",
            "💾 Saved checkpoint at epoch 1\n",
            "🌟 New best model saved at epoch 1 with acc 35.97%\n",
            "Epoch [  2/200] LR 0.09998 | Train Acc: 30.12% | Test Acc: 45.79% | MixUp: 148, CutMix: 157, None: 86\n",
            "💾 Saved checkpoint at epoch 2\n",
            "🌟 New best model saved at epoch 2 with acc 45.79%\n",
            "Epoch [  3/200] LR 0.09995 | Train Acc: 35.93% | Test Acc: 51.59% | MixUp: 139, CutMix: 161, None: 91\n",
            "💾 Saved checkpoint at epoch 3\n",
            "🌟 New best model saved at epoch 3 with acc 51.59%\n",
            "Epoch [  4/200] LR 0.09990 | Train Acc: 39.21% | Test Acc: 60.36% | MixUp: 163, CutMix: 163, None: 65\n",
            "💾 Saved checkpoint at epoch 4\n",
            "🌟 New best model saved at epoch 4 with acc 60.36%\n",
            "Epoch [  5/200] LR 0.09985 | Train Acc: 44.36% | Test Acc: 59.33% | MixUp: 164, CutMix: 153, None: 74\n",
            "💾 Saved checkpoint at epoch 5\n",
            "Epoch [  6/200] LR 0.09978 | Train Acc: 47.48% | Test Acc: 68.57% | MixUp: 160, CutMix: 157, None: 74\n",
            "💾 Saved checkpoint at epoch 6\n",
            "🌟 New best model saved at epoch 6 with acc 68.57%\n",
            "Epoch [  7/200] LR 0.09970 | Train Acc: 50.70% | Test Acc: 73.85% | MixUp: 153, CutMix: 159, None: 79\n",
            "💾 Saved checkpoint at epoch 7\n",
            "🌟 New best model saved at epoch 7 with acc 73.85%\n",
            "Epoch [  8/200] LR 0.09961 | Train Acc: 51.71% | Test Acc: 75.80% | MixUp: 154, CutMix: 163, None: 74\n",
            "💾 Saved checkpoint at epoch 8\n",
            "🌟 New best model saved at epoch 8 with acc 75.80%\n",
            "Epoch [  9/200] LR 0.09951 | Train Acc: 54.95% | Test Acc: 72.57% | MixUp: 170, CutMix: 133, None: 88\n",
            "💾 Saved checkpoint at epoch 9\n",
            "Epoch [ 10/200] LR 0.09939 | Train Acc: 55.84% | Test Acc: 75.81% | MixUp: 142, CutMix: 167, None: 82\n",
            "💾 Saved checkpoint at epoch 10\n",
            "🌟 New best model saved at epoch 10 with acc 75.81%\n",
            "Epoch [ 11/200] LR 0.09926 | Train Acc: 58.61% | Test Acc: 75.48% | MixUp: 153, CutMix: 155, None: 83\n",
            "💾 Saved checkpoint at epoch 11\n",
            "Epoch [ 12/200] LR 0.09912 | Train Acc: 54.57% | Test Acc: 74.35% | MixUp: 154, CutMix: 167, None: 70\n",
            "💾 Saved checkpoint at epoch 12\n",
            "Epoch [ 13/200] LR 0.09897 | Train Acc: 57.24% | Test Acc: 78.30% | MixUp: 146, CutMix: 181, None: 64\n",
            "💾 Saved checkpoint at epoch 13\n",
            "🌟 New best model saved at epoch 13 with acc 78.30%\n",
            "Epoch [ 14/200] LR 0.09881 | Train Acc: 57.54% | Test Acc: 78.68% | MixUp: 167, CutMix: 151, None: 73\n",
            "💾 Saved checkpoint at epoch 14\n",
            "🌟 New best model saved at epoch 14 with acc 78.68%\n",
            "Epoch [ 15/200] LR 0.09863 | Train Acc: 58.61% | Test Acc: 78.75% | MixUp: 143, CutMix: 171, None: 77\n",
            "💾 Saved checkpoint at epoch 15\n",
            "🌟 New best model saved at epoch 15 with acc 78.75%\n",
            "Epoch [ 16/200] LR 0.09844 | Train Acc: 58.24% | Test Acc: 80.20% | MixUp: 159, CutMix: 152, None: 80\n",
            "💾 Saved checkpoint at epoch 16\n",
            "🌟 New best model saved at epoch 16 with acc 80.20%\n",
            "Epoch [ 17/200] LR 0.09825 | Train Acc: 58.50% | Test Acc: 80.30% | MixUp: 161, CutMix: 161, None: 69\n",
            "💾 Saved checkpoint at epoch 17\n",
            "🌟 New best model saved at epoch 17 with acc 80.30%\n",
            "Epoch [ 18/200] LR 0.09803 | Train Acc: 56.85% | Test Acc: 73.03% | MixUp: 174, CutMix: 148, None: 69\n",
            "💾 Saved checkpoint at epoch 18\n",
            "Epoch [ 19/200] LR 0.09781 | Train Acc: 60.06% | Test Acc: 81.90% | MixUp: 148, CutMix: 152, None: 91\n",
            "💾 Saved checkpoint at epoch 19\n",
            "🌟 New best model saved at epoch 19 with acc 81.90%\n",
            "Epoch [ 20/200] LR 0.09758 | Train Acc: 56.30% | Test Acc: 83.49% | MixUp: 171, CutMix: 144, None: 76\n",
            "💾 Saved checkpoint at epoch 20\n",
            "🌟 New best model saved at epoch 20 with acc 83.49%\n",
            "Epoch [ 21/200] LR 0.09733 | Train Acc: 62.22% | Test Acc: 81.12% | MixUp: 147, CutMix: 148, None: 96\n",
            "💾 Saved checkpoint at epoch 21\n",
            "Epoch [ 22/200] LR 0.09707 | Train Acc: 61.40% | Test Acc: 83.91% | MixUp: 156, CutMix: 158, None: 77\n",
            "💾 Saved checkpoint at epoch 22\n",
            "🌟 New best model saved at epoch 22 with acc 83.91%\n",
            "Epoch [ 23/200] LR 0.09680 | Train Acc: 61.56% | Test Acc: 81.92% | MixUp: 142, CutMix: 156, None: 93\n",
            "💾 Saved checkpoint at epoch 23\n",
            "Epoch [ 24/200] LR 0.09652 | Train Acc: 61.61% | Test Acc: 80.96% | MixUp: 168, CutMix: 140, None: 83\n",
            "💾 Saved checkpoint at epoch 24\n",
            "Epoch [ 25/200] LR 0.09623 | Train Acc: 61.39% | Test Acc: 78.18% | MixUp: 154, CutMix: 153, None: 84\n",
            "💾 Saved checkpoint at epoch 25\n",
            "Epoch [ 26/200] LR 0.09593 | Train Acc: 56.18% | Test Acc: 80.44% | MixUp: 172, CutMix: 154, None: 65\n",
            "💾 Saved checkpoint at epoch 26\n",
            "Epoch [ 27/200] LR 0.09561 | Train Acc: 63.73% | Test Acc: 81.84% | MixUp: 140, CutMix: 161, None: 90\n",
            "💾 Saved checkpoint at epoch 27\n",
            "Epoch [ 28/200] LR 0.09529 | Train Acc: 59.44% | Test Acc: 80.27% | MixUp: 176, CutMix: 131, None: 84\n",
            "💾 Saved checkpoint at epoch 28\n",
            "Epoch [ 29/200] LR 0.09495 | Train Acc: 62.37% | Test Acc: 84.07% | MixUp: 142, CutMix: 166, None: 83\n",
            "💾 Saved checkpoint at epoch 29\n",
            "🌟 New best model saved at epoch 29 with acc 84.07%\n",
            "Epoch [ 30/200] LR 0.09460 | Train Acc: 61.60% | Test Acc: 84.78% | MixUp: 148, CutMix: 168, None: 75\n",
            "💾 Saved checkpoint at epoch 30\n",
            "🌟 New best model saved at epoch 30 with acc 84.78%\n",
            "Epoch [ 31/200] LR 0.09425 | Train Acc: 60.06% | Test Acc: 86.46% | MixUp: 164, CutMix: 160, None: 67\n",
            "💾 Saved checkpoint at epoch 31\n",
            "🌟 New best model saved at epoch 31 with acc 86.46%\n",
            "Epoch [ 32/200] LR 0.09388 | Train Acc: 61.39% | Test Acc: 85.22% | MixUp: 157, CutMix: 147, None: 87\n",
            "💾 Saved checkpoint at epoch 32\n",
            "Epoch [ 33/200] LR 0.09350 | Train Acc: 65.71% | Test Acc: 84.19% | MixUp: 149, CutMix: 145, None: 97\n",
            "💾 Saved checkpoint at epoch 33\n",
            "Epoch [ 34/200] LR 0.09311 | Train Acc: 61.75% | Test Acc: 85.25% | MixUp: 169, CutMix: 150, None: 72\n",
            "💾 Saved checkpoint at epoch 34\n",
            "Epoch [ 35/200] LR 0.09271 | Train Acc: 61.94% | Test Acc: 84.88% | MixUp: 155, CutMix: 152, None: 84\n",
            "💾 Saved checkpoint at epoch 35\n",
            "Epoch [ 36/200] LR 0.09229 | Train Acc: 61.30% | Test Acc: 81.91% | MixUp: 167, CutMix: 141, None: 83\n",
            "💾 Saved checkpoint at epoch 36\n",
            "Epoch [ 37/200] LR 0.09187 | Train Acc: 60.27% | Test Acc: 82.24% | MixUp: 161, CutMix: 164, None: 66\n",
            "💾 Saved checkpoint at epoch 37\n",
            "Epoch [ 38/200] LR 0.09144 | Train Acc: 64.59% | Test Acc: 80.80% | MixUp: 151, CutMix: 148, None: 92\n",
            "💾 Saved checkpoint at epoch 38\n",
            "Epoch [ 39/200] LR 0.09100 | Train Acc: 62.91% | Test Acc: 83.28% | MixUp: 139, CutMix: 171, None: 81\n",
            "💾 Saved checkpoint at epoch 39\n",
            "Epoch [ 40/200] LR 0.09055 | Train Acc: 59.98% | Test Acc: 84.54% | MixUp: 148, CutMix: 170, None: 73\n",
            "💾 Saved checkpoint at epoch 40\n",
            "Epoch [ 41/200] LR 0.09008 | Train Acc: 61.80% | Test Acc: 87.50% | MixUp: 155, CutMix: 151, None: 85\n",
            "💾 Saved checkpoint at epoch 41\n",
            "🌟 New best model saved at epoch 41 with acc 87.50%\n",
            "Epoch [ 42/200] LR 0.08961 | Train Acc: 61.63% | Test Acc: 86.05% | MixUp: 159, CutMix: 149, None: 83\n",
            "💾 Saved checkpoint at epoch 42\n",
            "Epoch [ 43/200] LR 0.08913 | Train Acc: 63.32% | Test Acc: 85.16% | MixUp: 148, CutMix: 156, None: 87\n",
            "💾 Saved checkpoint at epoch 43\n",
            "Epoch [ 44/200] LR 0.08864 | Train Acc: 62.16% | Test Acc: 84.74% | MixUp: 150, CutMix: 159, None: 82\n",
            "💾 Saved checkpoint at epoch 44\n",
            "Epoch [ 45/200] LR 0.08814 | Train Acc: 61.03% | Test Acc: 84.31% | MixUp: 171, CutMix: 146, None: 74\n",
            "💾 Saved checkpoint at epoch 45\n",
            "Epoch [ 46/200] LR 0.08763 | Train Acc: 61.67% | Test Acc: 83.93% | MixUp: 156, CutMix: 167, None: 68\n",
            "💾 Saved checkpoint at epoch 46\n",
            "Epoch [ 47/200] LR 0.08711 | Train Acc: 62.09% | Test Acc: 86.11% | MixUp: 171, CutMix: 143, None: 77\n",
            "💾 Saved checkpoint at epoch 47\n",
            "Epoch [ 48/200] LR 0.08658 | Train Acc: 59.95% | Test Acc: 85.08% | MixUp: 154, CutMix: 168, None: 69\n",
            "💾 Saved checkpoint at epoch 48\n",
            "Epoch [ 49/200] LR 0.08605 | Train Acc: 60.80% | Test Acc: 79.32% | MixUp: 169, CutMix: 143, None: 79\n",
            "💾 Saved checkpoint at epoch 49\n",
            "Epoch [ 50/200] LR 0.08550 | Train Acc: 60.61% | Test Acc: 86.38% | MixUp: 159, CutMix: 147, None: 85\n",
            "💾 Saved checkpoint at epoch 50\n",
            "Epoch [ 51/200] LR 0.08495 | Train Acc: 61.66% | Test Acc: 82.03% | MixUp: 164, CutMix: 150, None: 77\n",
            "💾 Saved checkpoint at epoch 51\n",
            "Epoch [ 52/200] LR 0.08439 | Train Acc: 62.37% | Test Acc: 84.44% | MixUp: 166, CutMix: 152, None: 73\n",
            "💾 Saved checkpoint at epoch 52\n",
            "Epoch [ 53/200] LR 0.08381 | Train Acc: 61.91% | Test Acc: 84.52% | MixUp: 161, CutMix: 136, None: 94\n",
            "💾 Saved checkpoint at epoch 53\n",
            "Epoch [ 54/200] LR 0.08323 | Train Acc: 62.28% | Test Acc: 88.79% | MixUp: 156, CutMix: 155, None: 80\n",
            "💾 Saved checkpoint at epoch 54\n",
            "🌟 New best model saved at epoch 54 with acc 88.79%\n",
            "Epoch [ 55/200] LR 0.08265 | Train Acc: 61.84% | Test Acc: 86.65% | MixUp: 161, CutMix: 148, None: 82\n",
            "💾 Saved checkpoint at epoch 55\n",
            "Epoch [ 56/200] LR 0.08205 | Train Acc: 60.38% | Test Acc: 87.01% | MixUp: 178, CutMix: 149, None: 64\n",
            "💾 Saved checkpoint at epoch 56\n",
            "Epoch [ 57/200] LR 0.08145 | Train Acc: 64.44% | Test Acc: 85.05% | MixUp: 134, CutMix: 178, None: 79\n",
            "💾 Saved checkpoint at epoch 57\n",
            "Epoch [ 58/200] LR 0.08084 | Train Acc: 61.51% | Test Acc: 86.19% | MixUp: 177, CutMix: 145, None: 69\n",
            "💾 Saved checkpoint at epoch 58\n",
            "Epoch [ 59/200] LR 0.08022 | Train Acc: 64.36% | Test Acc: 85.55% | MixUp: 158, CutMix: 149, None: 84\n",
            "💾 Saved checkpoint at epoch 59\n",
            "Epoch [ 60/200] LR 0.07960 | Train Acc: 60.59% | Test Acc: 84.83% | MixUp: 175, CutMix: 147, None: 69\n",
            "💾 Saved checkpoint at epoch 60\n",
            "Epoch [ 61/200] LR 0.07896 | Train Acc: 63.72% | Test Acc: 84.11% | MixUp: 147, CutMix: 153, None: 91\n",
            "💾 Saved checkpoint at epoch 61\n",
            "Epoch [ 62/200] LR 0.07832 | Train Acc: 61.08% | Test Acc: 86.11% | MixUp: 160, CutMix: 157, None: 74\n",
            "💾 Saved checkpoint at epoch 62\n",
            "Epoch [ 63/200] LR 0.07768 | Train Acc: 61.07% | Test Acc: 87.74% | MixUp: 164, CutMix: 154, None: 73\n",
            "💾 Saved checkpoint at epoch 63\n",
            "Epoch [ 64/200] LR 0.07702 | Train Acc: 64.53% | Test Acc: 87.28% | MixUp: 144, CutMix: 171, None: 76\n",
            "💾 Saved checkpoint at epoch 64\n",
            "Epoch [ 65/200] LR 0.07636 | Train Acc: 65.33% | Test Acc: 82.76% | MixUp: 157, CutMix: 156, None: 78\n",
            "💾 Saved checkpoint at epoch 65\n",
            "Epoch [ 66/200] LR 0.07570 | Train Acc: 62.90% | Test Acc: 86.07% | MixUp: 157, CutMix: 154, None: 80\n",
            "💾 Saved checkpoint at epoch 66\n",
            "Epoch [ 67/200] LR 0.07503 | Train Acc: 62.67% | Test Acc: 86.98% | MixUp: 153, CutMix: 166, None: 72\n",
            "💾 Saved checkpoint at epoch 67\n",
            "Epoch [ 68/200] LR 0.07435 | Train Acc: 63.10% | Test Acc: 87.44% | MixUp: 148, CutMix: 148, None: 95\n",
            "💾 Saved checkpoint at epoch 68\n",
            "Epoch [ 69/200] LR 0.07366 | Train Acc: 64.72% | Test Acc: 88.59% | MixUp: 152, CutMix: 167, None: 72\n",
            "💾 Saved checkpoint at epoch 69\n",
            "Epoch [ 70/200] LR 0.07297 | Train Acc: 64.35% | Test Acc: 85.81% | MixUp: 152, CutMix: 152, None: 87\n",
            "💾 Saved checkpoint at epoch 70\n",
            "Epoch [ 71/200] LR 0.07228 | Train Acc: 62.02% | Test Acc: 87.05% | MixUp: 158, CutMix: 166, None: 67\n",
            "💾 Saved checkpoint at epoch 71\n",
            "Epoch [ 72/200] LR 0.07158 | Train Acc: 63.04% | Test Acc: 85.56% | MixUp: 159, CutMix: 168, None: 64\n",
            "💾 Saved checkpoint at epoch 72\n",
            "Epoch [ 73/200] LR 0.07087 | Train Acc: 61.95% | Test Acc: 88.09% | MixUp: 150, CutMix: 167, None: 74\n",
            "💾 Saved checkpoint at epoch 73\n",
            "Epoch [ 74/200] LR 0.07016 | Train Acc: 61.94% | Test Acc: 88.54% | MixUp: 167, CutMix: 154, None: 70\n",
            "💾 Saved checkpoint at epoch 74\n",
            "Epoch [ 75/200] LR 0.06944 | Train Acc: 63.05% | Test Acc: 85.72% | MixUp: 148, CutMix: 157, None: 86\n",
            "💾 Saved checkpoint at epoch 75\n",
            "Epoch [ 76/200] LR 0.06872 | Train Acc: 61.62% | Test Acc: 89.91% | MixUp: 171, CutMix: 161, None: 59\n",
            "💾 Saved checkpoint at epoch 76\n",
            "🌟 New best model saved at epoch 76 with acc 89.91%\n",
            "Epoch [ 77/200] LR 0.06800 | Train Acc: 64.36% | Test Acc: 86.66% | MixUp: 140, CutMix: 166, None: 85\n",
            "💾 Saved checkpoint at epoch 77\n",
            "Epoch [ 78/200] LR 0.06727 | Train Acc: 63.70% | Test Acc: 88.37% | MixUp: 170, CutMix: 141, None: 80\n",
            "💾 Saved checkpoint at epoch 78\n",
            "Epoch [ 79/200] LR 0.06653 | Train Acc: 65.18% | Test Acc: 85.86% | MixUp: 153, CutMix: 147, None: 91\n",
            "💾 Saved checkpoint at epoch 79\n",
            "Epoch [ 80/200] LR 0.06580 | Train Acc: 61.27% | Test Acc: 87.47% | MixUp: 159, CutMix: 167, None: 65\n",
            "💾 Saved checkpoint at epoch 80\n",
            "Epoch [ 81/200] LR 0.06505 | Train Acc: 63.75% | Test Acc: 87.65% | MixUp: 150, CutMix: 175, None: 66\n",
            "💾 Saved checkpoint at epoch 81\n",
            "Epoch [ 82/200] LR 0.06431 | Train Acc: 63.91% | Test Acc: 86.55% | MixUp: 155, CutMix: 160, None: 76\n",
            "💾 Saved checkpoint at epoch 82\n",
            "Epoch [ 83/200] LR 0.06356 | Train Acc: 63.66% | Test Acc: 87.26% | MixUp: 158, CutMix: 144, None: 89\n",
            "💾 Saved checkpoint at epoch 83\n",
            "Epoch [ 84/200] LR 0.06281 | Train Acc: 64.29% | Test Acc: 88.25% | MixUp: 157, CutMix: 148, None: 86\n",
            "💾 Saved checkpoint at epoch 84\n",
            "Epoch [ 85/200] LR 0.06206 | Train Acc: 64.56% | Test Acc: 84.17% | MixUp: 168, CutMix: 146, None: 77\n",
            "💾 Saved checkpoint at epoch 85\n",
            "Epoch [ 86/200] LR 0.06130 | Train Acc: 64.92% | Test Acc: 87.42% | MixUp: 150, CutMix: 162, None: 79\n",
            "💾 Saved checkpoint at epoch 86\n",
            "Epoch [ 87/200] LR 0.06054 | Train Acc: 64.23% | Test Acc: 88.25% | MixUp: 160, CutMix: 172, None: 59\n",
            "💾 Saved checkpoint at epoch 87\n",
            "Epoch [ 88/200] LR 0.05978 | Train Acc: 65.82% | Test Acc: 88.38% | MixUp: 138, CutMix: 158, None: 95\n",
            "💾 Saved checkpoint at epoch 88\n",
            "Epoch [ 89/200] LR 0.05901 | Train Acc: 64.79% | Test Acc: 86.10% | MixUp: 164, CutMix: 143, None: 84\n",
            "💾 Saved checkpoint at epoch 89\n",
            "Epoch [ 90/200] LR 0.05824 | Train Acc: 63.14% | Test Acc: 87.35% | MixUp: 158, CutMix: 158, None: 75\n",
            "💾 Saved checkpoint at epoch 90\n",
            "Epoch [ 91/200] LR 0.05747 | Train Acc: 62.24% | Test Acc: 89.28% | MixUp: 145, CutMix: 171, None: 75\n",
            "💾 Saved checkpoint at epoch 91\n",
            "Epoch [ 92/200] LR 0.05670 | Train Acc: 64.37% | Test Acc: 85.56% | MixUp: 144, CutMix: 161, None: 86\n",
            "💾 Saved checkpoint at epoch 92\n",
            "Epoch [ 93/200] LR 0.05593 | Train Acc: 65.19% | Test Acc: 90.56% | MixUp: 157, CutMix: 175, None: 59\n",
            "💾 Saved checkpoint at epoch 93\n",
            "🌟 New best model saved at epoch 93 with acc 90.56%\n",
            "Epoch [ 94/200] LR 0.05516 | Train Acc: 65.72% | Test Acc: 89.14% | MixUp: 144, CutMix: 169, None: 78\n",
            "💾 Saved checkpoint at epoch 94\n",
            "Epoch [ 95/200] LR 0.05438 | Train Acc: 62.36% | Test Acc: 89.93% | MixUp: 158, CutMix: 151, None: 82\n",
            "💾 Saved checkpoint at epoch 95\n",
            "Epoch [ 96/200] LR 0.05361 | Train Acc: 66.74% | Test Acc: 89.75% | MixUp: 138, CutMix: 174, None: 79\n",
            "💾 Saved checkpoint at epoch 96\n",
            "Epoch [ 97/200] LR 0.05283 | Train Acc: 63.62% | Test Acc: 89.76% | MixUp: 150, CutMix: 161, None: 80\n",
            "💾 Saved checkpoint at epoch 97\n",
            "Epoch [ 98/200] LR 0.05205 | Train Acc: 64.92% | Test Acc: 89.12% | MixUp: 152, CutMix: 151, None: 88\n",
            "💾 Saved checkpoint at epoch 98\n",
            "Epoch [ 99/200] LR 0.05128 | Train Acc: 64.21% | Test Acc: 88.42% | MixUp: 157, CutMix: 158, None: 76\n",
            "💾 Saved checkpoint at epoch 99\n",
            "Epoch [100/200] LR 0.05050 | Train Acc: 67.60% | Test Acc: 85.85% | MixUp: 144, CutMix: 171, None: 76\n",
            "💾 Saved checkpoint at epoch 100\n",
            "Epoch [101/200] LR 0.04972 | Train Acc: 64.76% | Test Acc: 89.45% | MixUp: 154, CutMix: 164, None: 73\n",
            "💾 Saved checkpoint at epoch 101\n",
            "Epoch [102/200] LR 0.04895 | Train Acc: 66.09% | Test Acc: 89.56% | MixUp: 145, CutMix: 167, None: 79\n",
            "💾 Saved checkpoint at epoch 102\n",
            "Epoch [103/200] LR 0.04817 | Train Acc: 62.56% | Test Acc: 89.92% | MixUp: 154, CutMix: 155, None: 82\n",
            "💾 Saved checkpoint at epoch 103\n",
            "Epoch [104/200] LR 0.04739 | Train Acc: 65.95% | Test Acc: 90.01% | MixUp: 145, CutMix: 158, None: 88\n",
            "💾 Saved checkpoint at epoch 104\n",
            "Epoch [105/200] LR 0.04662 | Train Acc: 67.01% | Test Acc: 89.83% | MixUp: 138, CutMix: 159, None: 94\n",
            "💾 Saved checkpoint at epoch 105\n",
            "Epoch [106/200] LR 0.04584 | Train Acc: 63.68% | Test Acc: 87.82% | MixUp: 174, CutMix: 137, None: 80\n",
            "💾 Saved checkpoint at epoch 106\n",
            "Epoch [107/200] LR 0.04507 | Train Acc: 65.00% | Test Acc: 88.35% | MixUp: 160, CutMix: 152, None: 79\n",
            "💾 Saved checkpoint at epoch 107\n",
            "Epoch [108/200] LR 0.04430 | Train Acc: 64.55% | Test Acc: 89.73% | MixUp: 142, CutMix: 162, None: 87\n",
            "💾 Saved checkpoint at epoch 108\n",
            "Epoch [109/200] LR 0.04353 | Train Acc: 66.18% | Test Acc: 90.99% | MixUp: 152, CutMix: 147, None: 92\n",
            "💾 Saved checkpoint at epoch 109\n",
            "🌟 New best model saved at epoch 109 with acc 90.99%\n",
            "Epoch [110/200] LR 0.04276 | Train Acc: 63.54% | Test Acc: 90.07% | MixUp: 168, CutMix: 141, None: 82\n",
            "💾 Saved checkpoint at epoch 110\n",
            "Epoch [111/200] LR 0.04199 | Train Acc: 68.13% | Test Acc: 90.60% | MixUp: 156, CutMix: 148, None: 87\n",
            "💾 Saved checkpoint at epoch 111\n",
            "Epoch [112/200] LR 0.04122 | Train Acc: 63.00% | Test Acc: 88.21% | MixUp: 171, CutMix: 149, None: 71\n",
            "💾 Saved checkpoint at epoch 112\n",
            "Epoch [113/200] LR 0.04046 | Train Acc: 67.42% | Test Acc: 90.92% | MixUp: 152, CutMix: 149, None: 90\n",
            "💾 Saved checkpoint at epoch 113\n",
            "Epoch [114/200] LR 0.03970 | Train Acc: 65.90% | Test Acc: 90.12% | MixUp: 151, CutMix: 156, None: 84\n",
            "💾 Saved checkpoint at epoch 114\n",
            "Epoch [115/200] LR 0.03894 | Train Acc: 64.38% | Test Acc: 90.56% | MixUp: 160, CutMix: 150, None: 81\n",
            "💾 Saved checkpoint at epoch 115\n",
            "Epoch [116/200] LR 0.03819 | Train Acc: 67.32% | Test Acc: 89.80% | MixUp: 158, CutMix: 160, None: 73\n",
            "💾 Saved checkpoint at epoch 116\n",
            "Epoch [117/200] LR 0.03744 | Train Acc: 67.05% | Test Acc: 91.42% | MixUp: 154, CutMix: 153, None: 84\n",
            "💾 Saved checkpoint at epoch 117\n",
            "🌟 New best model saved at epoch 117 with acc 91.42%\n",
            "Epoch [118/200] LR 0.03669 | Train Acc: 67.27% | Test Acc: 89.60% | MixUp: 135, CutMix: 162, None: 94\n",
            "💾 Saved checkpoint at epoch 118\n",
            "Epoch [119/200] LR 0.03595 | Train Acc: 65.53% | Test Acc: 90.98% | MixUp: 163, CutMix: 147, None: 81\n",
            "💾 Saved checkpoint at epoch 119\n",
            "Epoch [120/200] LR 0.03520 | Train Acc: 65.84% | Test Acc: 88.86% | MixUp: 150, CutMix: 163, None: 78\n",
            "💾 Saved checkpoint at epoch 120\n",
            "Epoch [121/200] LR 0.03447 | Train Acc: 67.82% | Test Acc: 91.69% | MixUp: 156, CutMix: 143, None: 92\n",
            "💾 Saved checkpoint at epoch 121\n",
            "🌟 New best model saved at epoch 121 with acc 91.69%\n",
            "Epoch [122/200] LR 0.03373 | Train Acc: 66.99% | Test Acc: 90.88% | MixUp: 141, CutMix: 168, None: 82\n",
            "💾 Saved checkpoint at epoch 122\n",
            "Epoch [123/200] LR 0.03300 | Train Acc: 66.28% | Test Acc: 91.57% | MixUp: 152, CutMix: 162, None: 77\n",
            "💾 Saved checkpoint at epoch 123\n",
            "Epoch [124/200] LR 0.03228 | Train Acc: 67.27% | Test Acc: 91.07% | MixUp: 161, CutMix: 152, None: 78\n",
            "💾 Saved checkpoint at epoch 124\n",
            "Epoch [125/200] LR 0.03156 | Train Acc: 69.14% | Test Acc: 91.55% | MixUp: 143, CutMix: 165, None: 83\n",
            "💾 Saved checkpoint at epoch 125\n",
            "Epoch [126/200] LR 0.03084 | Train Acc: 65.87% | Test Acc: 91.16% | MixUp: 186, CutMix: 131, None: 74\n",
            "💾 Saved checkpoint at epoch 126\n",
            "Epoch [127/200] LR 0.03013 | Train Acc: 66.54% | Test Acc: 92.39% | MixUp: 151, CutMix: 162, None: 78\n",
            "💾 Saved checkpoint at epoch 127\n",
            "🌟 New best model saved at epoch 127 with acc 92.39%\n",
            "Epoch [128/200] LR 0.02942 | Train Acc: 64.27% | Test Acc: 92.61% | MixUp: 173, CutMix: 145, None: 73\n",
            "💾 Saved checkpoint at epoch 128\n",
            "🌟 New best model saved at epoch 128 with acc 92.61%\n",
            "Epoch [129/200] LR 0.02872 | Train Acc: 67.34% | Test Acc: 91.04% | MixUp: 172, CutMix: 144, None: 75\n",
            "💾 Saved checkpoint at epoch 129\n",
            "Epoch [130/200] LR 0.02803 | Train Acc: 68.22% | Test Acc: 91.38% | MixUp: 161, CutMix: 146, None: 84\n",
            "💾 Saved checkpoint at epoch 130\n",
            "Epoch [131/200] LR 0.02734 | Train Acc: 66.90% | Test Acc: 90.08% | MixUp: 160, CutMix: 162, None: 69\n",
            "💾 Saved checkpoint at epoch 131\n",
            "Epoch [132/200] LR 0.02665 | Train Acc: 69.12% | Test Acc: 92.29% | MixUp: 160, CutMix: 144, None: 87\n",
            "💾 Saved checkpoint at epoch 132\n",
            "Epoch [133/200] LR 0.02597 | Train Acc: 66.12% | Test Acc: 92.79% | MixUp: 178, CutMix: 148, None: 65\n",
            "💾 Saved checkpoint at epoch 133\n",
            "🌟 New best model saved at epoch 133 with acc 92.79%\n",
            "Epoch [134/200] LR 0.02530 | Train Acc: 68.65% | Test Acc: 92.13% | MixUp: 166, CutMix: 140, None: 85\n",
            "💾 Saved checkpoint at epoch 134\n",
            "Epoch [135/200] LR 0.02464 | Train Acc: 66.65% | Test Acc: 91.82% | MixUp: 152, CutMix: 176, None: 63\n",
            "💾 Saved checkpoint at epoch 135\n",
            "Epoch [136/200] LR 0.02398 | Train Acc: 66.42% | Test Acc: 92.77% | MixUp: 148, CutMix: 158, None: 85\n",
            "💾 Saved checkpoint at epoch 136\n",
            "Epoch [137/200] LR 0.02332 | Train Acc: 67.37% | Test Acc: 92.24% | MixUp: 169, CutMix: 150, None: 72\n",
            "💾 Saved checkpoint at epoch 137\n",
            "Epoch [138/200] LR 0.02268 | Train Acc: 69.02% | Test Acc: 93.43% | MixUp: 158, CutMix: 155, None: 78\n",
            "💾 Saved checkpoint at epoch 138\n",
            "🌟 New best model saved at epoch 138 with acc 93.43%\n",
            "Epoch [139/200] LR 0.02204 | Train Acc: 67.04% | Test Acc: 92.45% | MixUp: 163, CutMix: 159, None: 69\n",
            "💾 Saved checkpoint at epoch 139\n",
            "Epoch [140/200] LR 0.02140 | Train Acc: 69.29% | Test Acc: 93.17% | MixUp: 153, CutMix: 165, None: 73\n",
            "💾 Saved checkpoint at epoch 140\n",
            "Epoch [141/200] LR 0.02078 | Train Acc: 69.76% | Test Acc: 92.91% | MixUp: 162, CutMix: 144, None: 85\n",
            "💾 Saved checkpoint at epoch 141\n",
            "Epoch [142/200] LR 0.02016 | Train Acc: 69.18% | Test Acc: 92.36% | MixUp: 176, CutMix: 142, None: 73\n",
            "💾 Saved checkpoint at epoch 142\n",
            "Epoch [143/200] LR 0.01955 | Train Acc: 69.17% | Test Acc: 92.30% | MixUp: 144, CutMix: 175, None: 72\n",
            "💾 Saved checkpoint at epoch 143\n",
            "Epoch [144/200] LR 0.01895 | Train Acc: 69.71% | Test Acc: 93.55% | MixUp: 149, CutMix: 163, None: 79\n",
            "💾 Saved checkpoint at epoch 144\n",
            "🌟 New best model saved at epoch 144 with acc 93.55%\n",
            "Epoch [145/200] LR 0.01835 | Train Acc: 66.23% | Test Acc: 92.38% | MixUp: 157, CutMix: 163, None: 71\n",
            "💾 Saved checkpoint at epoch 145\n",
            "Epoch [146/200] LR 0.01777 | Train Acc: 67.80% | Test Acc: 93.54% | MixUp: 159, CutMix: 153, None: 79\n",
            "💾 Saved checkpoint at epoch 146\n",
            "Epoch [147/200] LR 0.01719 | Train Acc: 66.98% | Test Acc: 92.79% | MixUp: 154, CutMix: 161, None: 76\n",
            "💾 Saved checkpoint at epoch 147\n",
            "Epoch [148/200] LR 0.01661 | Train Acc: 70.80% | Test Acc: 93.07% | MixUp: 164, CutMix: 146, None: 81\n",
            "💾 Saved checkpoint at epoch 148\n",
            "Epoch [149/200] LR 0.01605 | Train Acc: 69.44% | Test Acc: 93.57% | MixUp: 160, CutMix: 154, None: 77\n",
            "💾 Saved checkpoint at epoch 149\n",
            "🌟 New best model saved at epoch 149 with acc 93.57%\n",
            "Epoch [150/200] LR 0.01550 | Train Acc: 70.66% | Test Acc: 93.37% | MixUp: 174, CutMix: 138, None: 79\n",
            "💾 Saved checkpoint at epoch 150\n",
            "Epoch [151/200] LR 0.01495 | Train Acc: 68.63% | Test Acc: 93.82% | MixUp: 154, CutMix: 165, None: 72\n",
            "💾 Saved checkpoint at epoch 151\n",
            "🌟 New best model saved at epoch 151 with acc 93.82%\n",
            "Epoch [152/200] LR 0.01442 | Train Acc: 70.15% | Test Acc: 93.51% | MixUp: 154, CutMix: 145, None: 92\n",
            "💾 Saved checkpoint at epoch 152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix & Misclassifications**"
      ],
      "metadata": {
        "id": "vDfaV8a6r1SA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sZFuvKmr56n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion & Analysis**"
      ],
      "metadata": {
        "id": "qd9s7dVBcmlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used:\n",
        "\n",
        " *Loss Function*\n",
        " - CrossEntropyLoss with Label Smoothing:\n",
        "- Standard CE compares predicted logits vs. one‑hot labels.\n",
        "- Label smoothing (e.g., 0.1) softens the target distribution: instead of [0,0,1,0,...], the true class gets 0.9 and others share 0.1.\n",
        "- Benefits: prevents overconfidence, improves calibration, helps generalization.\n",
        "\n",
        "\n",
        "*Optimizer*\n",
        "- SGD with Momentum (0.9) + Nesterov:\n",
        "- SGD updates weights in the direction of the gradient.\n",
        "- Momentum accumulates past gradients → smoother, faster convergence.\n",
        "- Nesterov momentum looks ahead, correcting overshoot.\n",
        "\n",
        "\n",
        "*Learning Rate Scheduler*\n",
        "- CosineAnnealingLR:\n",
        "- Starts at base LR (0.1).\n",
        "- Decays smoothly following a cosine curve toward eta_min (0.001).\n",
        "- Prevents sudden drops, encourages better minima.\n",
        "\n",
        "\n",
        "*Weight Decay*\n",
        "- L2 regularization: adds a penalty proportional to the square of weights.\n",
        "- Prevents weights from growing too large, reduces overfitting.\n",
        "- Standard value 5e‑4.\n",
        "\n",
        "\n",
        "\n",
        "*CutMix and MixUp*\n",
        "- MixUp: blends two images and their labels linearly. Encourages smooth decision boundaries.\n",
        "- CutMix: replaces a patch of one image with another, labels mixed by patch area. Preserves natural textures.\n",
        "- MixUp teaches interpolation, CutMix teaches occlusion robustness. Alternating gives the model both benefits.\n",
        "- Tapering: Strong mixing early (0.4, 0.4, 0.2), weaker late (0.15, 0.15, 0.70) so the model sharpens on real labels.\n"
      ],
      "metadata": {
        "id": "Dd5dxYGscqqp"
      }
    }
  ]
}